{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srihimavardhan/SENTIMENT_ANALYSIS/blob/main/CodTech_Task_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jzI4ndjynPv2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VzcOMvGpcsO",
        "outputId": "024f8a63-9d65-45f8-d21a-45209d3067e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentAnalyzer:\n",
        "    def __init__(self):\n",
        "        # Configure logging\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        # Initialize NLTK resources\n",
        "        self._initialize_nltk()\n",
        "\n",
        "        # Initialize preprocessing tools\n",
        "        self.tokenizer = RegexpTokenizer(r'\\w+')  # Use RegexpTokenizer instead of word_tokenize\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.model = None\n",
        "\n",
        "    def _initialize_nltk(self):\n",
        "        \"\"\"Download required NLTK resources safely\"\"\"\n",
        "        try:\n",
        "            nltk.download('stopwords', quiet=True)\n",
        "            nltk.download('wordnet', quiet=True)\n",
        "            nltk.download('omw-1.4', quiet=True)  # Required for newer versions of NLTK\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error downloading NLTK resources: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def create_sample_data(self):\n",
        "        \"\"\"Create a sample dataset of movie reviews\"\"\"\n",
        "        reviews = [\n",
        "            \"This movie was fantastic! Great acting and storyline\",\n",
        "            \"Terrible waste of time. Poor acting and boring plot\",\n",
        "            \"Really enjoyed this film, would watch again\",\n",
        "            \"Not worth the money, very disappointed\",\n",
        "            \"Average movie, nothing special but okay\",\n",
        "            \"One of the best movies I've ever seen\",\n",
        "            \"Complete disaster, avoid at all costs\",\n",
        "            \"Pretty good entertainment value\",\n",
        "            \"Absolutely loved every minute of it\",\n",
        "            \"Could have been better, somewhat disappointing\"\n",
        "        ]\n",
        "\n",
        "        ratings = [5, 1, 4, 1, 3, 5, 1, 4, 5, 2]\n",
        "\n",
        "        return pd.DataFrame({\n",
        "            'text': reviews,\n",
        "            'rating': ratings\n",
        "        })\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Clean and preprocess text data\"\"\"\n",
        "        try:\n",
        "            # Convert to lowercase and remove special characters\n",
        "            text = str(text).lower()\n",
        "            text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "            # Tokenize using RegexpTokenizer\n",
        "            tokens = self.tokenizer.tokenize(text)\n",
        "\n",
        "            # Remove stopwords and lemmatize\n",
        "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens\n",
        "                     if token not in self.stop_words and len(token) > 2]\n",
        "\n",
        "            return ' '.join(tokens)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error preprocessing text: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"Prepare data for training\"\"\"\n",
        "        self.logger.info(\"Preprocessing text data...\")\n",
        "\n",
        "        # Create copy to avoid modifying original\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Preprocess reviews\n",
        "        df_processed['processed_text'] = df_processed['text'].apply(self.preprocess_text)\n",
        "\n",
        "        # Convert ratings to sentiment\n",
        "        df_processed['sentiment'] = df_processed['rating'].apply(\n",
        "            lambda x: 'negative' if x <= 2 else 'positive' if x >= 4 else 'neutral'\n",
        "        )\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def train_model(self, X_train, y_train):\n",
        "        \"\"\"Train the sentiment analysis model\"\"\"\n",
        "        try:\n",
        "            self.logger.info(\"Training model...\")\n",
        "\n",
        "            # Create pipeline with improved parameters\n",
        "            self.model = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(\n",
        "                    max_features=5000,\n",
        "                    min_df=2,\n",
        "                    max_df=0.95,\n",
        "                    ngram_range=(1, 2)\n",
        "                )),\n",
        "                ('classifier', LogisticRegression(\n",
        "                    max_iter=1000,\n",
        "                    class_weight='balanced',\n",
        "                    random_state=42\n",
        "                ))\n",
        "            ])\n",
        "\n",
        "            self.model.fit(X_train, y_train)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error training model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def evaluate_model(self, X_test, y_test):\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained yet\")\n",
        "\n",
        "        try:\n",
        "            # Make predictions\n",
        "            y_pred = self.model.predict(X_test)\n",
        "\n",
        "            # Print classification report\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(classification_report(y_test, y_pred))\n",
        "\n",
        "            # Plot confusion matrix\n",
        "            self._plot_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error evaluating model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _plot_confusion_matrix(self, y_true, y_pred):\n",
        "        \"\"\"Plot confusion matrix\"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.show()\n",
        "\n",
        "    def analyze_new_text(self, texts):\n",
        "        \"\"\"Analyze sentiment of new texts\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained yet\")\n",
        "\n",
        "        try:\n",
        "            # Ensure texts is a list\n",
        "            if isinstance(texts, str):\n",
        "                texts = [texts]\n",
        "\n",
        "            # Preprocess new texts\n",
        "            processed_texts = [self.preprocess_text(text) for text in texts]\n",
        "\n",
        "            # Predict sentiments\n",
        "            predictions = self.model.predict(processed_texts)\n",
        "            probabilities = self.model.predict_proba(processed_texts)\n",
        "\n",
        "            # Create results dataframe\n",
        "            results = pd.DataFrame({\n",
        "                'text': texts,\n",
        "                'sentiment': predictions,\n",
        "                'confidence': np.max(probabilities, axis=1)\n",
        "            })\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error analyzing new text: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "F9Na9k9ZtmEh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    try:\n",
        "        # Initialize analyzer\n",
        "        analyzer = SentimentAnalyzer()\n",
        "\n",
        "        # Create sample data\n",
        "        df = analyzer.create_sample_data()\n",
        "\n",
        "        # Print dataset info\n",
        "        print(\"\\nDataset Info:\")\n",
        "        print(df.info())\n",
        "        print(\"\\nSample of raw data:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Prepare data\n",
        "        df_processed = analyzer.prepare_data(df)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            df_processed['processed_text'],\n",
        "            df_processed['sentiment'],\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=df_processed['sentiment']  # Ensure balanced split\n",
        "        )\n",
        "\n",
        "        # Train and evaluate model\n",
        "        analyzer.train_model(X_train, y_train)\n",
        "        analyzer.evaluate_model(X_test, y_test)\n",
        "\n",
        "        # Test with new reviews\n",
        "        new_reviews = [\n",
        "            \"This product is amazing! I absolutely love it!\",\n",
        "            \"Terrible experience, would not recommend to anyone.\",\n",
        "            \"It's okay, nothing special but gets the job done.\"\n",
        "        ]\n",
        "\n",
        "        results = analyzer.analyze_new_text(new_reviews)\n",
        "        print(\"\\nSentiment Analysis Results for New Reviews:\")\n",
        "        print(results.to_string(index=False))\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in main: {str(e)}\", exc_info=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33Brf8IGtxnG",
        "outputId": "76f30618-9c36-4bb7-cdca-a71835285acb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    10 non-null     object\n",
            " 1   rating  10 non-null     int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 292.0+ bytes\n",
            "None\n",
            "\n",
            "Sample of raw data:\n",
            "                                                text  rating\n",
            "0  This movie was fantastic! Great acting and sto...       5\n",
            "1  Terrible waste of time. Poor acting and boring...       1\n",
            "2        Really enjoyed this film, would watch again       4\n",
            "3             Not worth the money, very disappointed       1\n",
            "4            Average movie, nothing special but okay       3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:An error occurred in main: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-4-4820913fb441>\", line 19, in main\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "                                       ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\", line 2872, in train_test_split\n",
            "    train, test = next(cv.split(X=arrays[0], y=stratify))\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\", line 1909, in split\n",
            "    for train, test in self._iter_indices(X, y, groups):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\", line 2318, in _iter_indices\n",
            "    raise ValueError(\n",
            "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YqgT_IQgt-g8"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}